A mechanism in transformers that allows each position in the encoder to attend over all positions in the previous layer of the encoder, improving the model's ability to understand context.
---
Self-Attention
