A technique that normalizes the activations of the previous layers in a network to improve training stability and performance, commonly used in transformers.
---
Layer Normalization
