An architecture that utilizes self-attention to process all elements of the input sequence simultaneously and independently, enhancing the model's efficiency and effectiveness in understanding complex dependencies in data.
---
Transformer Architecture
