A technique where a smaller model (student) is trained to replicate the performance of a larger, pre-trained model (teacher) by learning to mimic its outputs, enhancing efficiency and potentially reducing overfitting.
---
Knowledge Distillation
