A parameter-efficient fine-tuning technique that uses low-rank transformations to update a subset of model weights, enhancing performance on specific tasks.
---
Low-rank Adaptation (LoRA)
