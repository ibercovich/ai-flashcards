An attention mechanism that allows a model to focus on different parts of an input sequence for each step of the output sequence. Developed to improve machine translation by addressing long sentence comprehension issues, it involves a separate RNN to compute attention weights.
---
Bahdanau Attention
