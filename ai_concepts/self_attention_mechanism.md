A key feature of transformers that allows each token in a sequence to interact with all other tokens, providing a comprehensive context-aware representation that enhances the model's understanding of the entire sequence.
---
Self-Attention Mechanism
