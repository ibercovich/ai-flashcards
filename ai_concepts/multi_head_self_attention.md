An advanced form of self-attention mechanism that allows the transformer to jointly attend to information from different representation subspaces at different positions.
---
Multi-Head Self-Attention
