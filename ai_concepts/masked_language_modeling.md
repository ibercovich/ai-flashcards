A pretraining objective where random words are masked or replaced in the input, and the model predicts the original token, used to train models like BERT.
---
Masked Language Modeling
