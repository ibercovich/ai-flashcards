A method of fine-tuning where a model is adjusted based on human feedback to align with user preferences, using techniques like reinforcement learning.
---
Reinforcement Learning with Human Feedback (RLHF)
