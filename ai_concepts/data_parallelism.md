A common approach where the training data is split into smaller batches that are processed in parallel across multiple GPUs, each holding a copy of the model. It maximizes hardware utilization but increases memory requirements.
---
Data Parallelism
