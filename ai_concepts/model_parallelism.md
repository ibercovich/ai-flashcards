A method where different parts of a neural network model are placed on different GPUs, allowing for the training of large models that exceed the memory capacity of a single GPU but can introduce inefficiencies due to sequential dependencies.
---
Model Parallelism
